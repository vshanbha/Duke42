
quarkus.langchain4j.timeout=180s
# Ollama Config
quarkus.langchain4j.chat-model.provider=ollama
quarkus.langchain4j.ollama.base-url=http://localhost:11434

# Chat model
quarkus.langchain4j.ollama.chat-model.model-name=llama3.2:1b
quarkus.langchain4j.ollama.chat-model.temperature=0.2
quarkus.langchain4j.ollama.log-requests=true

# Chat Memory
quarkus.langchain4j.chat-memory.type=MESSAGE_WINDOW
quarkus.langchain4j.chat-memory.memory-window.max-messages=10

# TOKEN_WINDOW estimator - does not work for Ollama
# Would require custom TokenCountEstimator to be built.
# quarkus.langchain4j.chat-memory.type=TOKEN_WINDOW

# Token Window for qwen3 family
# quarkus.langchain4j.chat-memory.token-window.max-tokens=40000

# Smaller Token window for smollm2 family
# quarkus.langchain4j.chat-memory.token-window.max-tokens=8000

# MCP Toolbox Config
# quarkus.langchain4j.mcp.default.transport-type=http
# quarkus.langchain4j.mcp.default.url=http://localhost:9000/mcp/sse