
quarkus.langchain4j.timeout=180s
# Ollama Config
quarkus.langchain4j.chat-model.provider=ollama
quarkus.langchain4j.ollama.base-url=http://localhost:11434

# Chat model
quarkus.langchain4j.ollama.chat-model.model-id=smollm2:1.7b
# quarkus.langchain4j.ollama.chat-model.model-name=qwen3:0.6b
quarkus.langchain4j.ollama.chat-model.temperature=0.2

# Chat Memory
quarkus.langchain4j.chat-memory.type=MESSAGE_WINDOW
quarkus.langchain4j.chat-memory.memory-window.max-messages=10

# TOKEN_WINDOW estimator - does not work for Ollama
# Would require custom TokenCountEstimator to be built.
# quarkus.langchain4j.chat-memory.type=TOKEN_WINDOW

# Token Window for qwen3 family
# quarkus.langchain4j.chat-memory.token-window.max-tokens=40000

# Smaller Token window for smollm2 family
# quarkus.langchain4j.chat-memory.token-window.max-tokens=8000